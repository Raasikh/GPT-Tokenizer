# ğŸ§  GPT Tokenizer - Byte Pair Encoding from Scratch

Welcome to **GPT Tokenizer** â€” a simple and educational Byte Pair Encoding (BPE) tokenizer built entirely from scratch using PyTorch and Python. This tokenizer demonstrates how subword tokenization (as used in GPT models) works under the hood.

---

## ğŸš€ Features

- âœ… Byte-level tokenization (256 base tokens)
- âœ… Customizable vocabulary size
- âœ… BPE merge logic implemented from scratch
- âœ… Encoding and decoding support
- âœ… Fully reversible token-to-text pipeline
- âœ… Educational, readable code structure

---

## ğŸ§© What is Byte Pair Encoding (BPE)?

Byte Pair Encoding is a subword tokenization method used in GPT and other Transformer models. It works by:

1. Starting with individual bytes (0â€“255)
2. Iteratively merging the most frequent adjacent token pairs
3. Building a vocabulary of base bytes + learned subwords

This leads to compact token sequences and efficient learning.

---

## ğŸ› ï¸ How It Works

### 1. Token Initialization
- Start with UTF-8 encoded bytes
- Each byte (0â€“255) is treated as a unique token

### 2. Merge Operations
- Count the most frequent adjacent token pairs
- Merge them to create new tokens
- Repeat until desired vocab size is reached

### 3. Encoding
```python
encoded = encode("hello world")
# -> [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]
```

### 4. Decoding
```python
decoded = decode(encoded)
# -> "hello world"
```

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ tokenizer.py         # Core BPE logic: merge, encode, decode
â”œâ”€â”€ input.txt            # Training corpus for building merge rules
â”œâ”€â”€ README.md            # You're here!
```

---

## ğŸ§ª Example Usage

```python
from tokenizer import encode, decode

text = "unbelievable machines"
encoded = encode(text)
decoded = decode(encoded)

print("Original:", text)
print("Encoded:", encoded)
print("Decoded:", decoded)
```

---

## ğŸ“š Learning Goals

This repo is built to help you:

- Understand BPE as used in GPT and LLM tokenizers
- Learn how vocab size impacts compression
- Explore how models tokenize and reconstruct text

---

## ğŸ“¦ Requirements

- Python 3.7+
- PyTorch (optional, if used for tensor-based optimization)

---

## ğŸ™Œ Credits

Inspired by the teaching style of [Andrej Karpathy](https://github.com/karpathy)  
Created with â¤ï¸ for learning and fun.

---
